# -*- coding: utf-8 -*-
"""twisting_squares_extra_stages.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o2ZFOXo6MKQvx51kjhyWGz7gFkrLIgnH
"""

# Commented out IPython magic to ensure Python compatibility.
# ==========================================================
#           COMPATIBILITY FIX FOR NUMPY & OPENCV
# ==========================================================
# %pip uninstall -y numpy opencv-python-headless opencv-python
# %pip install numpy==1.26.4 opencv-python==4.6.0.66 opencv-python-headless==4.6.0.66

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# # Clean up Colab's default environment and clone the repository if not already present
# if [ ! -d ".git" ]; then
#     rm -rf * .*
#     git clone -b master https://github.com/RyannDaGreat/Diffusion-Illusions .
# fi

# Commented out IPython magic to ensure Python compatibility.
# Fix version pins for compatibility
!sed -i 's/pandas==1.4.3/pandas==2.2.2/' requirements.txt
!sed -i 's/matplotlib==3.5.3/matplotlib==3.8.4/' requirements.txt
!sed -i 's/tqdm==4.64.0/tqdm==4.66.4/' requirements.txt

# Install dependencies
# %pip install --upgrade --no-deps -r requirements.txt
# %pip install pandas==2.2.2 tqdm==4.66.4 matplotlib==3.8.4 --upgrade
# %pip install rp --upgrade
# %pip install transformers -U

# -*- coding: utf-8 -*-
"""
Twisting Squares Script with Progressive Fourier Upscaling
- Preserves latent Fourier state between stages
- Upscales in place (blurry step is expected) and refines in next iterations
"""

# ==========================================================
#                CONFIGURATION PARAMETERS
# ============== ============================================
prompt_a = "photorealistic detail Frankenstein"
prompt_b = "a detailed wolfman"
negative_prompt = "cartoonish, blurry, unrecognizable"

NUMBER_OF_SQUARES = 4

STAGES = [
         {"size": 16,  "iterations": 100},
         {"size": 32,  "iterations": 250},
         {"size": 64,  "iterations": 500},
         {"size": 128,  "iterations": 500},
         {"size": 256, "iterations": 1000}
#         {"size": 512, "iterations": 2000}
]

GUIDANCE_SCALE = 100
NOISE_COEF = 0.125
DISPLAY_INTERVAL = 100

RUN_NAME = f"{prompt_a}-{prompt_b}"
SAVE_FOLDER = f"untracked/parker_puzzle_runs/{RUN_NAME}"

ZIP_OUTPUT = True
SAVE_TO_DRIVE = False
DRIVE_PATH = "/content/drive/MyDrive/Twisting_Squares"

# ==========================================================
#                       IMPORTS
# ==========================================================
import os, time, shutil
import numpy as np
import torch
import torch.nn as nn
import torch.utils.checkpoint as checkpoint
import cv2
from rp import *
import source.stable_diffusion as sd
from source.learnable_textures import LearnableImageFourier
from source.stable_diffusion_labels import NegativeLabel

assert torch.cuda.is_available(), "GPU runtime is required! (Runtime > Change runtime type > GPU)"

if 's' not in dir():
    model_name = "runwayml/stable-diffusion-v1-5"
    gpu = rp.select_torch_device()
    s = sd.StableDiffusion(gpu, model_name)
device = s.device

label_a = NegativeLabel(prompt_a, negative_prompt)
label_b = NegativeLabel(prompt_b, negative_prompt)

# ==========================================================
#                  IMAGE UTILS & ROTATION
# ==========================================================
def rotate_tiles(image, num_divisions=NUMBER_OF_SQUARES):
    image = as_torch_image(image)
    num_channels, height, width = image.shape
    tile_size = width // num_divisions
    output = torch.zeros_like(image)
    for x in range(num_divisions):
        for y in range(num_divisions):
            tile = image[:, y*tile_size:(y+1)*tile_size, x*tile_size:(x+1)*tile_size]
            tile = tile.rot90(1, [1, 2]) if (x + y) % 2 == 0 else tile.rot90(-1, [1, 2])
            output[:, y*tile_size:(y+1)*tile_size, x*tile_size:(x+1)*tile_size] = tile
    return output

def laplacian_enhance(image_np, alpha=0.3):
    if image_np.dtype != np.uint8:
        if image_np.max() <= 1.0:
            image_np = (np.clip(image_np, 0, 1) * 255).astype(np.uint8)
        else:
            image_np = np.clip(image_np, 0, 255).astype(np.uint8)
    if len(image_np.shape) == 2 or image_np.shape[2] == 1:
        image_np = cv2.cvtColor(image_np, cv2.COLOR_GRAY2BGR)
    elif image_np.shape[2] == 4:
        image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2BGR)
    else:
        image_np = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
    laplace_img = cv2.Laplacian(image_np, cv2.CV_32F)
    laplace_img = cv2.convertScaleAbs(laplace_img)
    enhanced = cv2.addWeighted(image_np, 1.0, laplace_img, alpha, 0)
    return enhanced

# ==========================================================
#          TRAINING UTILITIES & DISPLAY FUNCTIONS
# ==========================================================
def make_learnable_image(size):
    return LearnableImageFourier(height=size, width=size, num_features=256, hidden_dim=256, scale=10).to(device)

def upscale_learnable_image(image, new_size):
    """
    In-place upscale of LearnableImageFourier, preserving latent state.
    """
    with torch.no_grad():
        if hasattr(image, "features"):
            param = image.features
        else:
            param = next(image.parameters())

        # Ensure 4D shape
        if param.dim() == 3:
            param_4d = param.unsqueeze(0)
        else:
            param_4d = param

        # Interpolate
        upscaled_param = torch.nn.functional.interpolate(
            param_4d,
            size=(new_size, new_size),
            mode='bilinear',
            align_corners=False
        )

        if param.dim() == 3:
            upscaled_param = upscaled_param.squeeze(0)

        # Replace parameter
        if hasattr(image, "features"):
            image.features = torch.nn.Parameter(upscaled_param)
        else:
            for name, _ in image.named_parameters():
                setattr(image, name, torch.nn.Parameter(upscaled_param))
                break

        # Update internal dimensions
        if hasattr(image, "height"):
            image.height = new_size
        if hasattr(image, "width"):
            image.width = new_size

    return image

def get_display_image(img_a, img_b):
    return rp.tiled_images(
        [rp.as_numpy_image(img_a()), rp.as_numpy_image(img_b())],
        length=2, border_thickness=0
    )

def save_run_images(name, images):
    folder = f"{SAVE_FOLDER}/{name}"
    if rp.path_exists(folder):
        folder += f"_{int(time.time())}"
    rp.make_directory(folder)
    ims_names = [f"ims_{i:04d}.png" for i in range(len(images))]
    with rp.SetCurrentDirectoryTemporarily(folder):
        rp.save_images(images, ims_names, show_progress=True)
    print(f"Saved timelapse to: {folder}")
    return folder

# ==========================================================
#                       MAIN LOGIC
# ==========================================================
ims = []
weights = rp.as_numpy_array([1, 1])
weights = (weights / weights.sum()) * len(weights)

# Start at smallest resolution
image = make_learnable_image(STAGES[0]["size"])

# Add checkpointing to LearnableImageFourier forward pass
class LearnableImageFourierWithCheckpointing(LearnableImageFourier):
    def forward(self, condition=None):
        features = self.get_features(condition)
        # Apply checkpointing to the model's forward pass
        output = checkpoint.checkpoint(self.model, features, use_reentrant=False).squeeze(0)
        assert output.shape==(self.num_channels, self.height, self.width)
        return output

image = LearnableImageFourierWithCheckpointing(height=STAGES[0]["size"], width=STAGES[0]["size"], num_features=256, hidden_dim=256, scale=10).to(device)


for stage_idx, stage in enumerate(STAGES):
    SIZE = stage["size"]
    NUM_ITER = stage["iterations"]

    # Upscale in place between stages
    if stage_idx > 0:
        print(f"Upscaling existing learnable image to {SIZE}x{SIZE}")
        image = upscale_learnable_image(image, SIZE)

    learnable_image_a = lambda: image()
    learnable_image_b = lambda: rotate_tiles(image(), NUMBER_OF_SQUARES)
    optim = torch.optim.SGD(image.parameters(), lr=1e-4)

    print(f"\n--- Stage {stage_idx+1}/{len(STAGES)} ---")
    print(f"Resolution: {SIZE}x{SIZE}, Iterations: {NUM_ITER}")

    television = rp.JupyterDisplayChannel()
    television.display()
    display_eta = rp.eta(NUM_ITER, title=f'Stage {stage_idx+1}')

    for iter_num in range(NUM_ITER):
        display_eta(iter_num)
        for label, learnable_img, weight in rp.random_batch(list(zip([label_a, label_b], [learnable_image_a, learnable_image_b], weights)), batch_size=1):
            s.train_step(label.embedding, learnable_img()[None], noise_coef=NOISE_COEF * weight, guidance_scale=GUIDANCE_SCALE)
        with torch.no_grad():
            if not iter_num % (DISPLAY_INTERVAL // 4):
                im = get_display_image(learnable_image_a, learnable_image_b)
                ims.append(im)
                television.update(im)
                if not iter_num % DISPLAY_INTERVAL:
                    rp.display_image(im)
        optim.step()
        optim.zero_grad()

    current_img = rp.as_numpy_image(learnable_image_a())
    current_img = cv2.cvtColor(current_img, cv2.COLOR_RGB2BGR)
    current_img = laplacian_enhance(current_img)
    if stage_idx < len(STAGES) - 1:
        upscaled = cv2.resize(current_img, (STAGES[stage_idx+1]["size"], STAGES[stage_idx+1]["size"]), interpolation=cv2.INTER_CUBIC)
    else:
        upscaled = current_img

# ==========================================================
#                     SAVE & EXPORT
# ==========================================================
print("Final Image (Unsolved):")
rp.display_image(rp.as_numpy_image(learnable_image_a()))
print("Final Image (Solved):")
rp.display_image(rp.as_numpy_image(learnable_image_b()))

folder_path = save_run_images(RUN_NAME, ims)

if ZIP_OUTPUT:
    shutil.make_archive(RUN_NAME, 'zip', folder_path)
    from google.colab import files
    files.download(f"{RUN_NAME}.zip")

#if SAVE_TO_DRIVE:
#    from google.colab import drive
#    drive.mount('/content/drive')
#    !mkdir -p "$DRIVE_PATH"
#    !cp -r "{folder_path}" "$DRIVE_PATH"

# === Optional: Continue training for extra stages ===
EXTRA_STAGE_COUNT = 1  # Set to number of additional stages to run
EXTRA_STAGE_ITERATIONS = 4000  # Iterations per extra stage

if EXTRA_STAGE_COUNT > 0:
    for extra_stage in range(EXTRA_STAGE_COUNT):
        print(f"\n--- Extra Stage {extra_stage+1}/{EXTRA_STAGE_COUNT} ---")
        print(f"Resolution: {image.width}x{image.height}, Iterations: {EXTRA_STAGE_ITERATIONS}")

        learnable_image_a = lambda: image()
        learnable_image_b = lambda: rotate_tiles(image(), NUMBER_OF_SQUARES)
        optim = torch.optim.SGD(image.parameters(), lr=1e-4)

        television = rp.JupyterDisplayChannel()
        television.display()
        display_eta = rp.eta(EXTRA_STAGE_ITERATIONS, title=f'Extra Stage {extra_stage+1}')

        for iter_num in range(EXTRA_STAGE_ITERATIONS):
            display_eta(iter_num)
            for label, learnable_img, weight in rp.random_batch(
                list(zip([label_a, label_b], [learnable_image_a, learnable_image_b], weights)), batch_size=1):
                s.train_step(label.embedding, learnable_img()[None],
                             noise_coef=NOISE_COEF * weight, guidance_scale=GUIDANCE_SCALE)
            with torch.no_grad():
                if not iter_num % (DISPLAY_INTERVAL // 4):
                    im = get_display_image(learnable_image_a, learnable_image_b)
                    ims.append(im)
                    television.update(im)
                    if not iter_num % DISPLAY_INTERVAL:
                        rp.display_image(im)
            optim.step()
            optim.zero_grad()

    print("Extra training complete.")


print("Final Image (Unsolved):")
rp.display_image(rp.as_numpy_image(learnable_image_a()))
print("Final Image (Solved):")
rp.display_image(rp.as_numpy_image(learnable_image_b()))

folder_path = save_run_images(RUN_NAME, ims)

if ZIP_OUTPUT:
    shutil.make_archive(RUN_NAME, 'zip', folder_path)
    from google.colab import files
    files.download(f"{RUN_NAME}.zip")

# === Optional: Continue training for extra stages ===
EXTRA_STAGE_COUNT = 1  # Set to number of additional stages to run
EXTRA_STAGE_ITERATIONS = 4000  # Iterations per extra stage

if EXTRA_STAGE_COUNT > 0:
    for extra_stage in range(EXTRA_STAGE_COUNT):
        print(f"\n--- Extra Stage {extra_stage+1}/{EXTRA_STAGE_COUNT} ---")
        print(f"Resolution: {image.width}x{image.height}, Iterations: {EXTRA_STAGE_ITERATIONS}")

        learnable_image_a = lambda: image()
        learnable_image_b = lambda: rotate_tiles(image(), NUMBER_OF_SQUARES)
        optim = torch.optim.SGD(image.parameters(), lr=1e-4)

        television = rp.JupyterDisplayChannel()
        television.display()
        display_eta = rp.eta(EXTRA_STAGE_ITERATIONS, title=f'Extra Stage {extra_stage+1}')

        for iter_num in range(EXTRA_STAGE_ITERATIONS):
            display_eta(iter_num)
            for label, learnable_img, weight in rp.random_batch(
                list(zip([label_a, label_b], [learnable_image_a, learnable_image_b], weights)), batch_size=1):
                s.train_step(label.embedding, learnable_img()[None],
                             noise_coef=NOISE_COEF * weight, guidance_scale=GUIDANCE_SCALE)
            with torch.no_grad():
                if not iter_num % (DISPLAY_INTERVAL // 4):
                    im = get_display_image(learnable_image_a, learnable_image_b)
                    ims.append(im)
                    television.update(im)
                    if not iter_num % DISPLAY_INTERVAL:
                        rp.display_image(im)
            optim.step()
            optim.zero_grad()

    print("Extra training complete.")


print("Final Image (Unsolved):")
rp.display_image(rp.as_numpy_image(learnable_image_a()))
print("Final Image (Solved):")
rp.display_image(rp.as_numpy_image(learnable_image_b()))

folder_path = save_run_images(RUN_NAME, ims)

if ZIP_OUTPUT:
    shutil.make_archive(RUN_NAME, 'zip', folder_path)
    from google.colab import files
    files.download(f"{RUN_NAME}.zip")